{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QrZqmW-YtUKw"
   },
   "source": [
    "# Keras Basics\n",
    "We will learn about\n",
    "* Dense layers\n",
    "* Categorical cross-entropy\n",
    "\n",
    "A toy example to show how to train a classifier with Keras and use it. The data comes from three gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eWyR310TtUKy"
   },
   "outputs": [],
   "source": [
    "## DATA GENERATION\n",
    "import numpy as np\n",
    "\n",
    "def generateX(cls):\n",
    "    '''\n",
    "    Inputs:\n",
    "        cls: class {0, 1, 2}\n",
    "    Outputs:\n",
    "        x: a sample from cls; a np array of shape (2,)\n",
    "    '''\n",
    "    assert cls in [0,1,2]\n",
    "    \n",
    "    if cls==0:\n",
    "        x = np.random.normal(np.array([0,0]),100)\n",
    "    elif cls==1:\n",
    "        x = np.random.normal(np.array([200,200]),100)\n",
    "    elif cls==2:\n",
    "        x = np.random.normal(np.array([-200,200]),100)\n",
    "    return x\n",
    "Nx = 2 # shape of a sample is (2,)\n",
    "Ny = 3 # 3 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24sOXTIttUK2"
   },
   "source": [
    "Could you write a function to generate N samples from class 0 and N samples from class 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "9W3ccvYYtUK4",
    "nbgrader": {
     "checksum": "71c5837a9d68fac4398e11bcb87c3bd2",
     "grade": false,
     "grade_id": "cell-6ee804e3860f2ff6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def generateXY(N):\n",
    "    '''\n",
    "    Inputs:\n",
    "        N: no. of samples of each class\n",
    "    Outputs:\n",
    "        X: np array of samples; shape = (3*N, 2)\n",
    "        Y: np array of samples; shape = (3*N, 1)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    X = np.ones((3*N,2))\n",
    "    \n",
    "    for j in range(3*N):\n",
    "      if(j<N):\n",
    "        for k in range(N):\n",
    "          X[k,:] = generateX(0)\n",
    "        continue\n",
    "      if(j<2*N):\n",
    "        for k in range(N):\n",
    "          X[k,:] = generateX(1)\n",
    "        continue\n",
    "      if(j<3*N):\n",
    "        for k in range(N):\n",
    "          X[k,:] = generateX(2)\n",
    "        continue\n",
    "    \n",
    "    Y = np.ones((3*N,1))\n",
    "    \n",
    "    for i in range(3*N):\n",
    "      if(i<N):\n",
    "        Y[i,0]=0\n",
    "        continue\n",
    "      if(i<2*N):\n",
    "        Y[i,0]=1\n",
    "        continue\n",
    "      else:\n",
    "        Y[i,0] = 2\n",
    "        continue\n",
    "      \n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 1888,
     "status": "ok",
     "timestamp": 1561106626217,
     "user": {
      "displayName": "Abhay Katheria",
      "photoUrl": "https://lh6.googleusercontent.com/-Pz70x0oam4U/AAAAAAAAAAI/AAAAAAAAAR0/9JRu1fsisCA/s64/photo.jpg",
      "userId": "15733656777530091355"
     },
     "user_tz": -330
    },
    "id": "sUqeO_NWtUK7",
    "nbgrader": {
     "checksum": "c0183471e369c049b734441886caaff4",
     "grade": true,
     "grade_id": "cell-ad908829419fd089",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "ff8c4920-69c3-4305-b0f6-cee2968c2110"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed 👍\n"
     ]
    }
   ],
   "source": [
    "def test_generateXY():\n",
    "    X_train, Y_train = generateXY(50)\n",
    "    assert X_train.shape==(150,2)\n",
    "    assert Y_train.shape==(150,1)\n",
    "    print('Test passed', '\\U0001F44D')\n",
    "test_generateXY()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJ3TNcj-tULI"
   },
   "source": [
    "### One-hot encoding\n",
    "\n",
    "Now our Y is in the form [0], [1] and [2]. We want to convert them to [1,0,0], [0,1,0] and [0,0,1], respectively. \n",
    "Could you write a code to convert Y (with one column) into one-hot encoded Y (with 3 columns)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "_n4fYMC0tULJ",
    "nbgrader": {
     "checksum": "2920fc139021b2f772982b2e16731703",
     "grade": false,
     "grade_id": "cell-db496b9b86c28424",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def oneHot(y, Ny):\n",
    "    '''\n",
    "    Input:\n",
    "        y: an int in {0, 1, 2}\n",
    "        Ny: Number of classes, e.g., 3 here.\n",
    "    Output:\n",
    "        Y: a vector of Ny (=3) tuples\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    Y = np.zeros((1,Ny))\n",
    "    for i in range(Ny):\n",
    "      if(i==y):\n",
    "        Y[0,i]=1\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 1834,
     "status": "ok",
     "timestamp": 1561106626231,
     "user": {
      "displayName": "Abhay Katheria",
      "photoUrl": "https://lh6.googleusercontent.com/-Pz70x0oam4U/AAAAAAAAAAI/AAAAAAAAAR0/9JRu1fsisCA/s64/photo.jpg",
      "userId": "15733656777530091355"
     },
     "user_tz": -330
    },
    "id": "fq8OZ0cxtULM",
    "nbgrader": {
     "checksum": "8612cec704a627b66ff552899569f828",
     "grade": true,
     "grade_id": "cell-24fb717c7ea66826",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "5813bb74-34db-428f-cf80-6b0fb4659743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed 👍\n"
     ]
    }
   ],
   "source": [
    "def test_oneHot():\n",
    "    assert np.all(oneHot(0,3)==np.array([1,0,0]))\n",
    "    assert np.all(oneHot(1,3)==np.array([0,1,0]))\n",
    "    assert np.all(oneHot(2,3)==np.array([0,0,1]))\n",
    "    print('Test passed', '\\U0001F44D')\n",
    "test_oneHot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EzQywDiStULR"
   },
   "source": [
    "### Input Normalization\n",
    "X can lie in any unbounded range. We need to curtail to a narrow range close to zero. This helps in enhancing the stability of training and hyper-parameter tuning.\n",
    "This is normally achieved by scaling the X to have zero mean and unit standard deviation (std).\n",
    "\n",
    "$X \\leftarrow \\frac{X-mean(X)}{std(X)}$, where this is element wise division\n",
    "\n",
    "Could you use training samples to find mean and std, and normalize your X_train with that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "_v_HEe10tULS",
    "nbgrader": {
     "checksum": "4d88f9abc4004f238b182e54336e76e2",
     "grade": false,
     "grade_id": "cell-8564364c76ddcdc7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def findMeanStddev(X):\n",
    "    '''\n",
    "    Input: \n",
    "        X: a matrix of size (no. of samples, dimension of each sample)\n",
    "    Output:\n",
    "        mean: mean of samples in X; shape is (dimension of each sample,)\n",
    "        stddev: element-wise std dev of sample in X; shape is (dimension of each sample,)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    mean = X.mean(axis=0)\n",
    "    stddev=X.std(axis=0)\n",
    "    return mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 1788,
     "status": "ok",
     "timestamp": 1561106626245,
     "user": {
      "displayName": "Abhay Katheria",
      "photoUrl": "https://lh6.googleusercontent.com/-Pz70x0oam4U/AAAAAAAAAAI/AAAAAAAAAR0/9JRu1fsisCA/s64/photo.jpg",
      "userId": "15733656777530091355"
     },
     "user_tz": -330
    },
    "id": "qYUioNiqxRyi",
    "nbgrader": {
     "checksum": "5d5ccf5b778b190a8607fe045aebce74",
     "grade": true,
     "grade_id": "cell-c060c271af9064e7",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "4777226c-3bfc-4e46-9fa5-c2a5e3bf7895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed 👍\n"
     ]
    }
   ],
   "source": [
    "def test_findMeanStddev():\n",
    "    X = np.array([[3,2,6],[7,4,2],[3,5,1]])\n",
    "    mean, stddev = findMeanStddev(X)\n",
    "    assert np.isclose(mean, np.array([4.33, 3.66, 3.]), atol=0.1).all()\n",
    "    assert np.isclose(stddev, np.array([1.88, 1.24, 2.16]), atol=0.1).all()\n",
    "    print('Test passed', '\\U0001F44D')\n",
    "test_findMeanStddev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "gG4sm2WfwRqu",
    "nbgrader": {
     "checksum": "6c5fc8807584cce426d6dd8fd10dfc7c",
     "grade": false,
     "grade_id": "cell-80ad17d9f5962f88",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def normalizeX(X, mean, stddev):\n",
    "    '''\n",
    "    Input:\n",
    "        X: a matrix of size (no. of samples, dimension of each sample)\n",
    "        mean: mean of samples in X (same size as X)\n",
    "        stddev: element-wise std dev of sample in X (same size as X) \n",
    "    Output:\n",
    "        Xn: X modified to have 0 mean and 1 std dev\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    Xn=X\n",
    "    if(stddev[0,] == 0):\n",
    "      Xn=Xn*0\n",
    "    else:\n",
    "       Xn=(X-mean)/stddev\n",
    "    return Xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 1748,
     "status": "ok",
     "timestamp": 1561106626262,
     "user": {
      "displayName": "Abhay Katheria",
      "photoUrl": "https://lh6.googleusercontent.com/-Pz70x0oam4U/AAAAAAAAAAI/AAAAAAAAAR0/9JRu1fsisCA/s64/photo.jpg",
      "userId": "15733656777530091355"
     },
     "user_tz": -330
    },
    "id": "t4JdFDb7tULZ",
    "nbgrader": {
     "checksum": "cb4af2655a94d3e991efe9c64ba57a8c",
     "grade": true,
     "grade_id": "cell-0880b9b53201680b",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    },
    "outputId": "a292e24f-5e85-4a1c-ef76-d0b25077875a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed 👍\n"
     ]
    }
   ],
   "source": [
    "def test_normalizeX():\n",
    "    X = np.ones((3,3))\n",
    "    m,s = findMeanStddev(X)\n",
    "    assert np.all(m==np.ones(3))\n",
    "    assert np.all(s==np.zeros(3))\n",
    "    assert np.all(normalizeX(X,m,s)==0*X)\n",
    "    # test on random X\n",
    "    X = np.random.random((5,3))\n",
    "    m,s = findMeanStddev(X)\n",
    "    Xn = normalizeX(X,m,s)\n",
    "    mn, sn = findMeanStddev(Xn)\n",
    "    assert np.allclose(mn, np.zeros(3))\n",
    "    assert np.allclose(sn, np.ones(3))\n",
    "    print('Test passed', '\\U0001F44D')\n",
    "test_normalizeX()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1zpEX8EEtULe"
   },
   "source": [
    "### Plotting\n",
    "Could you plot all the samples in X_train with different colors for different classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfkkWGWZtULf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "def plotXY(X, Y):\n",
    "    '''\n",
    "    Inputs:\n",
    "        X: a matrix of size (no. of samples, dimension of each sample)\n",
    "        Y: a matrix of size (no. of samples, no. of classes) - these are one-hot vectors\n",
    "    Action:\n",
    "        Plots the samples in X, their color depends on Y\n",
    "    '''\n",
    "    Ny = Y.shape[1]\n",
    "    for cls in range(Ny):\n",
    "        idx = np.where(Y[:,cls]==1)[0]\n",
    "        plt.plot(X[idx,0], X[idx,1], colors[cls]+'.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DL396wBwRL9B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kB3jK6IrtULk"
   },
   "source": [
    "## Creating the Network\n",
    "We now create the network with dense layers: \n",
    "$y = f(Wx)$\n",
    "\n",
    "ReLU activation: \n",
    "$f(h) = h, h>0; 0, h\\le 0$\n",
    "\n",
    "Softmax activation: \n",
    "$f(h_i) = \\frac{\\exp(h_i)}{\\sum_j \\exp(h_j)}$\n",
    "\n",
    "Categorical cross-entropy loss:\n",
    "$\\mathcal{L} = -\\sum_t y^d_t \\log y_t$\n",
    "\n",
    "Stochastic Gradient Descent:\n",
    "$w_{ij} \\leftarrow w_{ij} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_{ij}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "g60Qem92tULl",
    "nbgrader": {
     "checksum": "1f332e46663382a71f1dfa333725d807",
     "grade": false,
     "grade_id": "cell-e18133df577f7820",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "def makeNN(Nx, Nh, Ny):\n",
    "    '''\n",
    "    Input:\n",
    "        Nx: int; no. of input nodes; shape of each sample; i.e., X.shape[1:] \n",
    "        Nh: int; no. of hidden neurons\n",
    "        Ny: int; no. of output nodes; shape of output; i.e., Y.shape[1]\n",
    "    Output:\n",
    "        model: keras NN model with Input layer, Dense layer with Nh neurons, \n",
    "                and Dense output layer with softmax non-linearity, loss function\n",
    "                categorical-crossentropy, optimizer SGD.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    input_layer = Input(shape =( Nx,))\n",
    "    hidden_layer1 = Dense(Nh, activation = 'relu',)(input_layer)\n",
    "    hidden_layer2 = Dense(Nh, activation = 'tanh',)(hidden_layer1)\n",
    "    output_layer = Dense(Ny, activation = 'softmax')(hidden_layer2)\n",
    "    model = Model(inputs=[input_layer], outputs=[output_layer])\n",
    "    model.compile(optimizer=optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=True), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-Gf4DSltULt"
   },
   "source": [
    "### Plotting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qkPv1xptULu"
   },
   "outputs": [],
   "source": [
    "def plotModel(model):\n",
    "    from keras.utils import plot_model\n",
    "    plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
    "    from IPython.display import Image\n",
    "    Image(retina=True, filename='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fi_-l93RQ5cr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XO26y8VZtULz"
   },
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "Yfvm6pH5tUL0",
    "nbgrader": {
     "checksum": "78b5d80f2ec4bcbc57d80a6bc475f285",
     "grade": false,
     "grade_id": "cell-8a4f621147d44a84",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def trainNN(model, X_train, Y_train, Nepochs):\n",
    "    '''\n",
    "    Action:\n",
    "        Train model with model.fit\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    history = model.fit(X_train,Y_train,batch_size=10,epochs=Nepochs,validation_split = 0.4)\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.plot(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "QZRghwIG2I5c",
    "nbgrader": {
     "checksum": "5d34ead5470a39daddbd54aa7f19ef1f",
     "grade": false,
     "grade_id": "cell-c45fc6de4c3fc4c9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def trainModel(N, Nh, Nepochs):\n",
    "    '''\n",
    "    generateXY, normalizeX, oneHot, makeNN, trainNN\n",
    "    Input:\n",
    "        N: int; no. of training samples per class\n",
    "        Nh: int; no. of neurons in hidden layer\n",
    "    Output:\n",
    "        model: keras NN model trained with the training data\n",
    "        mean_train, stddev_train: mean and stddev of training data - you will \n",
    "                            need this for normalizing your test data\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    X,Y=generateXY(N)\n",
    "    plotXY(X, Y)\n",
    "    print(Y.shape)\n",
    "    m,s=findMeanStddev(X)\n",
    "    X=normalizeX(X,m,s)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    from keras.utils import to_categorical\n",
    "    Yc = to_categorical(Y)\n",
    "    print(Yc.shape)\n",
    "    train_frac = 0.8\n",
    "    train_samples = indices[:int(train_frac*X.shape[0])]\n",
    "    ## this is really awesome hes picking first 80% train indices then next others\n",
    "    test_samples = indices[int(train_frac*X.shape[0]):]\n",
    "    x_train = X[train_samples]\n",
    "    y_train = Yc[train_samples]\n",
    "\n",
    "    \n",
    "    x_test = X[test_samples]\n",
    "    y_test = Yc[test_samples]\n",
    "    yn_test = Y[test_samples]\n",
    "    \n",
    "    Nx=2\n",
    "    Ny=3\n",
    "    model=makeNN(Nx, Nh, Ny)\n",
    "    mean_train,stddev_train= findMeanStddev(X)\n",
    "    trainNN(model,x_train,y_train, Nepochs)\n",
    "     \n",
    "    \n",
    "    return model, mean_train, stddev_train,x_test,y_test,yn_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y7a1LEcgtUL4"
   },
   "source": [
    "### Evaluation\n",
    "Could you:\n",
    "- Generate 20 samples from each class\n",
    "- Normalize them with mean_train and stddev_train\n",
    "- Get Y_test as one hot encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 19396
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18163,
     "status": "ok",
     "timestamp": 1561108522577,
     "user": {
      "displayName": "Abhay Katheria",
      "photoUrl": "https://lh6.googleusercontent.com/-Pz70x0oam4U/AAAAAAAAAAI/AAAAAAAAAR0/9JRu1fsisCA/s64/photo.jpg",
      "userId": "15733656777530091355"
     },
     "user_tz": -330
    },
    "id": "N9AKg5YBtUMB",
    "outputId": "8281aa1b-1f54-4841-826c-331c2e9fbd37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1)\n",
      "(300, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 42)                126       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 42)                1806      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 3)                 129       \n",
      "=================================================================\n",
      "Total params: 2,061\n",
      "Trainable params: 2,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 144 samples, validate on 96 samples\n",
      "Epoch 1/500\n",
      "144/144 [==============================] - 1s 4ms/step - loss: 1.3028 - acc: 0.0069 - val_loss: 1.2586 - val_acc: 0.0104\n",
      "Epoch 2/500\n",
      "144/144 [==============================] - 0s 189us/step - loss: 1.3007 - acc: 0.0069 - val_loss: 1.2569 - val_acc: 0.0104\n",
      "Epoch 3/500\n",
      "144/144 [==============================] - 0s 179us/step - loss: 1.2985 - acc: 0.0069 - val_loss: 1.2551 - val_acc: 0.0104\n",
      "Epoch 4/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 1.2963 - acc: 0.0069 - val_loss: 1.2534 - val_acc: 0.0104\n",
      "Epoch 5/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 1.2940 - acc: 0.0069 - val_loss: 1.2517 - val_acc: 0.0104\n",
      "Epoch 6/500\n",
      "144/144 [==============================] - 0s 210us/step - loss: 1.2919 - acc: 0.0069 - val_loss: 1.2501 - val_acc: 0.0104\n",
      "Epoch 7/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 1.2898 - acc: 0.0069 - val_loss: 1.2484 - val_acc: 0.0104\n",
      "Epoch 8/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 1.2876 - acc: 0.0069 - val_loss: 1.2467 - val_acc: 0.0104\n",
      "Epoch 9/500\n",
      "144/144 [==============================] - 0s 179us/step - loss: 1.2855 - acc: 0.0069 - val_loss: 1.2450 - val_acc: 0.0104\n",
      "Epoch 10/500\n",
      "144/144 [==============================] - 0s 184us/step - loss: 1.2833 - acc: 0.0069 - val_loss: 1.2433 - val_acc: 0.0104\n",
      "Epoch 11/500\n",
      "144/144 [==============================] - 0s 219us/step - loss: 1.2812 - acc: 0.0069 - val_loss: 1.2417 - val_acc: 0.0104\n",
      "Epoch 12/500\n",
      "144/144 [==============================] - 0s 218us/step - loss: 1.2791 - acc: 0.0069 - val_loss: 1.2400 - val_acc: 0.0104\n",
      "Epoch 13/500\n",
      "144/144 [==============================] - 0s 307us/step - loss: 1.2770 - acc: 0.0069 - val_loss: 1.2384 - val_acc: 0.0104\n",
      "Epoch 14/500\n",
      "144/144 [==============================] - 0s 201us/step - loss: 1.2749 - acc: 0.0069 - val_loss: 1.2368 - val_acc: 0.0104\n",
      "Epoch 15/500\n",
      "144/144 [==============================] - 0s 193us/step - loss: 1.2728 - acc: 0.0069 - val_loss: 1.2351 - val_acc: 0.0104\n",
      "Epoch 16/500\n",
      "144/144 [==============================] - 0s 193us/step - loss: 1.2707 - acc: 0.0069 - val_loss: 1.2334 - val_acc: 0.0104\n",
      "Epoch 17/500\n",
      "144/144 [==============================] - 0s 220us/step - loss: 1.2685 - acc: 0.0069 - val_loss: 1.2318 - val_acc: 0.0000e+00\n",
      "Epoch 18/500\n",
      "144/144 [==============================] - 0s 197us/step - loss: 1.2664 - acc: 0.0069 - val_loss: 1.2302 - val_acc: 0.0000e+00\n",
      "Epoch 19/500\n",
      "144/144 [==============================] - 0s 209us/step - loss: 1.2644 - acc: 0.0069 - val_loss: 1.2285 - val_acc: 0.0000e+00\n",
      "Epoch 20/500\n",
      "144/144 [==============================] - 0s 215us/step - loss: 1.2623 - acc: 0.0069 - val_loss: 1.2269 - val_acc: 0.0000e+00\n",
      "Epoch 21/500\n",
      "144/144 [==============================] - 0s 189us/step - loss: 1.2602 - acc: 0.0069 - val_loss: 1.2253 - val_acc: 0.0000e+00\n",
      "Epoch 22/500\n",
      "144/144 [==============================] - 0s 194us/step - loss: 1.2582 - acc: 0.0069 - val_loss: 1.2237 - val_acc: 0.0000e+00\n",
      "Epoch 23/500\n",
      "144/144 [==============================] - 0s 194us/step - loss: 1.2561 - acc: 0.0069 - val_loss: 1.2221 - val_acc: 0.0000e+00\n",
      "Epoch 24/500\n",
      "144/144 [==============================] - 0s 203us/step - loss: 1.2541 - acc: 0.0069 - val_loss: 1.2205 - val_acc: 0.0000e+00\n",
      "Epoch 25/500\n",
      "144/144 [==============================] - 0s 196us/step - loss: 1.2520 - acc: 0.0069 - val_loss: 1.2189 - val_acc: 0.0000e+00\n",
      "Epoch 26/500\n",
      "144/144 [==============================] - 0s 189us/step - loss: 1.2499 - acc: 0.0069 - val_loss: 1.2173 - val_acc: 0.0000e+00\n",
      "Epoch 27/500\n",
      "144/144 [==============================] - 0s 205us/step - loss: 1.2479 - acc: 0.0069 - val_loss: 1.2157 - val_acc: 0.0000e+00\n",
      "Epoch 28/500\n",
      "144/144 [==============================] - 0s 252us/step - loss: 1.2458 - acc: 0.0069 - val_loss: 1.2141 - val_acc: 0.0000e+00\n",
      "Epoch 29/500\n",
      "144/144 [==============================] - 0s 192us/step - loss: 1.2438 - acc: 0.0069 - val_loss: 1.2125 - val_acc: 0.0000e+00\n",
      "Epoch 30/500\n",
      "144/144 [==============================] - 0s 191us/step - loss: 1.2418 - acc: 0.0069 - val_loss: 1.2110 - val_acc: 0.0000e+00\n",
      "Epoch 31/500\n",
      "144/144 [==============================] - 0s 200us/step - loss: 1.2399 - acc: 0.0069 - val_loss: 1.2094 - val_acc: 0.0000e+00\n",
      "Epoch 32/500\n",
      "144/144 [==============================] - 0s 193us/step - loss: 1.2378 - acc: 0.0069 - val_loss: 1.2078 - val_acc: 0.0000e+00\n",
      "Epoch 33/500\n",
      "144/144 [==============================] - 0s 194us/step - loss: 1.2358 - acc: 0.0069 - val_loss: 1.2063 - val_acc: 0.0000e+00\n",
      "Epoch 34/500\n",
      "144/144 [==============================] - 0s 198us/step - loss: 1.2339 - acc: 0.0069 - val_loss: 1.2047 - val_acc: 0.0000e+00\n",
      "Epoch 35/500\n",
      "144/144 [==============================] - 0s 195us/step - loss: 1.2318 - acc: 0.0069 - val_loss: 1.2031 - val_acc: 0.0000e+00\n",
      "Epoch 36/500\n",
      "144/144 [==============================] - 0s 191us/step - loss: 1.2298 - acc: 0.0069 - val_loss: 1.2016 - val_acc: 0.0000e+00\n",
      "Epoch 37/500\n",
      "144/144 [==============================] - 0s 187us/step - loss: 1.2279 - acc: 0.0069 - val_loss: 1.2000 - val_acc: 0.0000e+00\n",
      "Epoch 38/500\n",
      "144/144 [==============================] - 0s 203us/step - loss: 1.2258 - acc: 0.0069 - val_loss: 1.1985 - val_acc: 0.0000e+00\n",
      "Epoch 39/500\n",
      "144/144 [==============================] - 0s 183us/step - loss: 1.2239 - acc: 0.0069 - val_loss: 1.1970 - val_acc: 0.0000e+00\n",
      "Epoch 40/500\n",
      "144/144 [==============================] - 0s 192us/step - loss: 1.2220 - acc: 0.0069 - val_loss: 1.1955 - val_acc: 0.0000e+00\n",
      "Epoch 41/500\n",
      "144/144 [==============================] - 0s 190us/step - loss: 1.2200 - acc: 0.0069 - val_loss: 1.1940 - val_acc: 0.0000e+00\n",
      "Epoch 42/500\n",
      "144/144 [==============================] - 0s 200us/step - loss: 1.2181 - acc: 0.0069 - val_loss: 1.1924 - val_acc: 0.0000e+00\n",
      "Epoch 43/500\n",
      "144/144 [==============================] - 0s 192us/step - loss: 1.2162 - acc: 0.0069 - val_loss: 1.1909 - val_acc: 0.0000e+00\n",
      "Epoch 44/500\n",
      "144/144 [==============================] - 0s 201us/step - loss: 1.2143 - acc: 0.1319 - val_loss: 1.1894 - val_acc: 0.3646\n",
      "Epoch 45/500\n",
      "144/144 [==============================] - 0s 202us/step - loss: 1.2123 - acc: 0.3403 - val_loss: 1.1878 - val_acc: 0.3646\n",
      "Epoch 46/500\n",
      "144/144 [==============================] - 0s 202us/step - loss: 1.2103 - acc: 0.3403 - val_loss: 1.1864 - val_acc: 0.3646\n",
      "Epoch 47/500\n",
      "144/144 [==============================] - 0s 197us/step - loss: 1.2085 - acc: 0.3403 - val_loss: 1.1849 - val_acc: 0.3646\n",
      "Epoch 48/500\n",
      "144/144 [==============================] - 0s 203us/step - loss: 1.2066 - acc: 0.3403 - val_loss: 1.1834 - val_acc: 0.3646\n",
      "Epoch 49/500\n",
      "144/144 [==============================] - 0s 195us/step - loss: 1.2047 - acc: 0.3403 - val_loss: 1.1819 - val_acc: 0.3646\n",
      "Epoch 50/500\n",
      "144/144 [==============================] - 0s 198us/step - loss: 1.2028 - acc: 0.3403 - val_loss: 1.1805 - val_acc: 0.3646\n",
      "Epoch 51/500\n",
      "144/144 [==============================] - 0s 194us/step - loss: 1.2009 - acc: 0.3403 - val_loss: 1.1790 - val_acc: 0.3646\n",
      "Epoch 52/500\n",
      "144/144 [==============================] - 0s 193us/step - loss: 1.1991 - acc: 0.3403 - val_loss: 1.1776 - val_acc: 0.3646\n",
      "Epoch 53/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 1.1972 - acc: 0.3403 - val_loss: 1.1761 - val_acc: 0.3646\n",
      "Epoch 54/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 1.1954 - acc: 0.3403 - val_loss: 1.1747 - val_acc: 0.3646\n",
      "Epoch 55/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 1.1935 - acc: 0.3403 - val_loss: 1.1732 - val_acc: 0.3750\n",
      "Epoch 56/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 1.1917 - acc: 0.3403 - val_loss: 1.1718 - val_acc: 0.3750\n",
      "Epoch 57/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 1.1899 - acc: 0.3403 - val_loss: 1.1703 - val_acc: 0.3750\n",
      "Epoch 58/500\n",
      "144/144 [==============================] - 0s 160us/step - loss: 1.1880 - acc: 0.3403 - val_loss: 1.1689 - val_acc: 0.3750\n",
      "Epoch 59/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 1.1862 - acc: 0.3403 - val_loss: 1.1675 - val_acc: 0.3750\n",
      "Epoch 60/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 1.1844 - acc: 0.3403 - val_loss: 1.1661 - val_acc: 0.3750\n",
      "Epoch 61/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 1.1826 - acc: 0.3403 - val_loss: 1.1647 - val_acc: 0.3750\n",
      "Epoch 62/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 1.1809 - acc: 0.3542 - val_loss: 1.1632 - val_acc: 0.3750\n",
      "Epoch 63/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 1.1791 - acc: 0.3542 - val_loss: 1.1618 - val_acc: 0.3750\n",
      "Epoch 64/500\n",
      "144/144 [==============================] - 0s 196us/step - loss: 1.1773 - acc: 0.3542 - val_loss: 1.1604 - val_acc: 0.3750\n",
      "Epoch 65/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 1.1755 - acc: 0.3542 - val_loss: 1.1590 - val_acc: 0.3750\n",
      "Epoch 66/500\n",
      "144/144 [==============================] - 0s 179us/step - loss: 1.1737 - acc: 0.3542 - val_loss: 1.1576 - val_acc: 0.3750\n",
      "Epoch 67/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 1.1720 - acc: 0.3542 - val_loss: 1.1562 - val_acc: 0.3750\n",
      "Epoch 68/500\n",
      "144/144 [==============================] - 0s 189us/step - loss: 1.1703 - acc: 0.3542 - val_loss: 1.1547 - val_acc: 0.3750\n",
      "Epoch 69/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 1.1683 - acc: 0.3542 - val_loss: 1.1534 - val_acc: 0.3750\n",
      "Epoch 70/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 1.1666 - acc: 0.3542 - val_loss: 1.1520 - val_acc: 0.3750\n",
      "Epoch 71/500\n",
      "144/144 [==============================] - 0s 177us/step - loss: 1.1649 - acc: 0.3542 - val_loss: 1.1506 - val_acc: 0.3750\n",
      "Epoch 72/500\n",
      "144/144 [==============================] - 0s 177us/step - loss: 1.1631 - acc: 0.3611 - val_loss: 1.1493 - val_acc: 0.3750\n",
      "Epoch 73/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 1.1614 - acc: 0.3611 - val_loss: 1.1479 - val_acc: 0.3750\n",
      "Epoch 74/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 1.1596 - acc: 0.3611 - val_loss: 1.1465 - val_acc: 0.3750\n",
      "Epoch 75/500\n",
      "144/144 [==============================] - 0s 184us/step - loss: 1.1579 - acc: 0.3611 - val_loss: 1.1452 - val_acc: 0.3750\n",
      "Epoch 76/500\n",
      "144/144 [==============================] - 0s 186us/step - loss: 1.1562 - acc: 0.3611 - val_loss: 1.1438 - val_acc: 0.3854\n",
      "Epoch 77/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 1.1545 - acc: 0.3611 - val_loss: 1.1425 - val_acc: 0.3854\n",
      "Epoch 78/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 1.1529 - acc: 0.3611 - val_loss: 1.1411 - val_acc: 0.3854\n",
      "Epoch 79/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 1.1511 - acc: 0.3611 - val_loss: 1.1398 - val_acc: 0.3854\n",
      "Epoch 80/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 1.1494 - acc: 0.3611 - val_loss: 1.1384 - val_acc: 0.3854\n",
      "Epoch 81/500\n",
      "144/144 [==============================] - 0s 178us/step - loss: 1.1476 - acc: 0.3611 - val_loss: 1.1371 - val_acc: 0.3854\n",
      "Epoch 82/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 1.1460 - acc: 0.3611 - val_loss: 1.1358 - val_acc: 0.3854\n",
      "Epoch 83/500\n",
      "144/144 [==============================] - 0s 179us/step - loss: 1.1443 - acc: 0.3611 - val_loss: 1.1345 - val_acc: 0.3854\n",
      "Epoch 84/500\n",
      "144/144 [==============================] - 0s 180us/step - loss: 1.1427 - acc: 0.3611 - val_loss: 1.1332 - val_acc: 0.3854\n",
      "Epoch 85/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 1.1410 - acc: 0.3611 - val_loss: 1.1318 - val_acc: 0.3854\n",
      "Epoch 86/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 1.1394 - acc: 0.3611 - val_loss: 1.1305 - val_acc: 0.3854\n",
      "Epoch 87/500\n",
      "144/144 [==============================] - 0s 178us/step - loss: 1.1377 - acc: 0.3611 - val_loss: 1.1292 - val_acc: 0.3854\n",
      "Epoch 88/500\n",
      "144/144 [==============================] - 0s 175us/step - loss: 1.1360 - acc: 0.3611 - val_loss: 1.1279 - val_acc: 0.3854\n",
      "Epoch 89/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 1.1344 - acc: 0.3611 - val_loss: 1.1266 - val_acc: 0.3958\n",
      "Epoch 90/500\n",
      "144/144 [==============================] - 0s 177us/step - loss: 1.1327 - acc: 0.3611 - val_loss: 1.1253 - val_acc: 0.3958\n",
      "Epoch 91/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 1.1310 - acc: 0.3611 - val_loss: 1.1239 - val_acc: 0.3958\n",
      "Epoch 92/500\n",
      "144/144 [==============================] - 0s 164us/step - loss: 1.1294 - acc: 0.3611 - val_loss: 1.1227 - val_acc: 0.3958\n",
      "Epoch 93/500\n",
      "144/144 [==============================] - 0s 178us/step - loss: 1.1278 - acc: 0.3611 - val_loss: 1.1214 - val_acc: 0.3958\n",
      "Epoch 94/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 1.1261 - acc: 0.3611 - val_loss: 1.1201 - val_acc: 0.3958\n",
      "Epoch 95/500\n",
      "144/144 [==============================] - 0s 160us/step - loss: 1.1245 - acc: 0.3611 - val_loss: 1.1188 - val_acc: 0.3958\n",
      "Epoch 96/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 1.1229 - acc: 0.3611 - val_loss: 1.1175 - val_acc: 0.3958\n",
      "Epoch 97/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 1.1212 - acc: 0.3611 - val_loss: 1.1162 - val_acc: 0.3958\n",
      "Epoch 98/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 1.1196 - acc: 0.3611 - val_loss: 1.1149 - val_acc: 0.3958\n",
      "Epoch 99/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 1.1180 - acc: 0.3611 - val_loss: 1.1136 - val_acc: 0.3958\n",
      "Epoch 100/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 1.1164 - acc: 0.3611 - val_loss: 1.1124 - val_acc: 0.3958\n",
      "Epoch 101/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 1.1148 - acc: 0.3681 - val_loss: 1.1111 - val_acc: 0.3958\n",
      "Epoch 102/500\n",
      "144/144 [==============================] - 0s 187us/step - loss: 1.1132 - acc: 0.3681 - val_loss: 1.1099 - val_acc: 0.3958\n",
      "Epoch 103/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 1.1116 - acc: 0.3681 - val_loss: 1.1086 - val_acc: 0.3958\n",
      "Epoch 104/500\n",
      "144/144 [==============================] - 0s 164us/step - loss: 1.1100 - acc: 0.3681 - val_loss: 1.1074 - val_acc: 0.3958\n",
      "Epoch 105/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 1.1085 - acc: 0.3681 - val_loss: 1.1062 - val_acc: 0.3958\n",
      "Epoch 106/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 1.1070 - acc: 0.3681 - val_loss: 1.1050 - val_acc: 0.3958\n",
      "Epoch 107/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 1.1054 - acc: 0.3681 - val_loss: 1.1037 - val_acc: 0.3958\n",
      "Epoch 108/500\n",
      "144/144 [==============================] - 0s 160us/step - loss: 1.1039 - acc: 0.3681 - val_loss: 1.1025 - val_acc: 0.3958\n",
      "Epoch 109/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 1.1024 - acc: 0.3681 - val_loss: 1.1013 - val_acc: 0.3958\n",
      "Epoch 110/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 1.1009 - acc: 0.3681 - val_loss: 1.1001 - val_acc: 0.3958\n",
      "Epoch 111/500\n",
      "144/144 [==============================] - 0s 159us/step - loss: 1.0993 - acc: 0.3681 - val_loss: 1.0989 - val_acc: 0.3958\n",
      "Epoch 112/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 1.0978 - acc: 0.3681 - val_loss: 1.0977 - val_acc: 0.3958\n",
      "Epoch 113/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 1.0963 - acc: 0.3681 - val_loss: 1.0965 - val_acc: 0.3958\n",
      "Epoch 114/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 1.0948 - acc: 0.3819 - val_loss: 1.0953 - val_acc: 0.4063\n",
      "Epoch 115/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 1.0933 - acc: 0.3819 - val_loss: 1.0941 - val_acc: 0.4063\n",
      "Epoch 116/500\n",
      "144/144 [==============================] - 0s 182us/step - loss: 1.0918 - acc: 0.3819 - val_loss: 1.0929 - val_acc: 0.4063\n",
      "Epoch 117/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 1.0903 - acc: 0.3819 - val_loss: 1.0917 - val_acc: 0.4063\n",
      "Epoch 118/500\n",
      "144/144 [==============================] - 0s 179us/step - loss: 1.0888 - acc: 0.3819 - val_loss: 1.0905 - val_acc: 0.4063\n",
      "Epoch 119/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 1.0873 - acc: 0.3819 - val_loss: 1.0893 - val_acc: 0.4063\n",
      "Epoch 120/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 1.0858 - acc: 0.3819 - val_loss: 1.0881 - val_acc: 0.4063\n",
      "Epoch 121/500\n",
      "144/144 [==============================] - 0s 155us/step - loss: 1.0843 - acc: 0.3819 - val_loss: 1.0870 - val_acc: 0.4167\n",
      "Epoch 122/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 1.0828 - acc: 0.3889 - val_loss: 1.0858 - val_acc: 0.4167\n",
      "Epoch 123/500\n",
      "144/144 [==============================] - 0s 182us/step - loss: 1.0814 - acc: 0.3889 - val_loss: 1.0847 - val_acc: 0.4375\n",
      "Epoch 124/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 1.0800 - acc: 0.4028 - val_loss: 1.0835 - val_acc: 0.4479\n",
      "Epoch 125/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 1.0785 - acc: 0.4028 - val_loss: 1.0824 - val_acc: 0.4583\n",
      "Epoch 126/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 1.0771 - acc: 0.4028 - val_loss: 1.0812 - val_acc: 0.4688\n",
      "Epoch 127/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 1.0757 - acc: 0.4097 - val_loss: 1.0801 - val_acc: 0.4688\n",
      "Epoch 128/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 1.0743 - acc: 0.4097 - val_loss: 1.0790 - val_acc: 0.4792\n",
      "Epoch 129/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 1.0728 - acc: 0.4097 - val_loss: 1.0778 - val_acc: 0.5000\n",
      "Epoch 130/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 1.0714 - acc: 0.4236 - val_loss: 1.0767 - val_acc: 0.5104\n",
      "Epoch 131/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 1.0700 - acc: 0.4236 - val_loss: 1.0755 - val_acc: 0.5313\n",
      "Epoch 132/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 1.0685 - acc: 0.4236 - val_loss: 1.0744 - val_acc: 0.5313\n",
      "Epoch 133/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 1.0671 - acc: 0.4375 - val_loss: 1.0733 - val_acc: 0.5417\n",
      "Epoch 134/500\n",
      "144/144 [==============================] - 0s 164us/step - loss: 1.0657 - acc: 0.4444 - val_loss: 1.0721 - val_acc: 0.5521\n",
      "Epoch 135/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 1.0643 - acc: 0.4514 - val_loss: 1.0710 - val_acc: 0.5521\n",
      "Epoch 136/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 1.0629 - acc: 0.4653 - val_loss: 1.0699 - val_acc: 0.5521\n",
      "Epoch 137/500\n",
      "144/144 [==============================] - 0s 186us/step - loss: 1.0615 - acc: 0.4861 - val_loss: 1.0688 - val_acc: 0.5521\n",
      "Epoch 138/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 1.0601 - acc: 0.5000 - val_loss: 1.0677 - val_acc: 0.5521\n",
      "Epoch 139/500\n",
      "144/144 [==============================] - 0s 158us/step - loss: 1.0588 - acc: 0.5069 - val_loss: 1.0666 - val_acc: 0.5521\n",
      "Epoch 140/500\n",
      "144/144 [==============================] - 0s 183us/step - loss: 1.0574 - acc: 0.5139 - val_loss: 1.0655 - val_acc: 0.5521\n",
      "Epoch 141/500\n",
      "144/144 [==============================] - 0s 175us/step - loss: 1.0560 - acc: 0.5139 - val_loss: 1.0644 - val_acc: 0.5521\n",
      "Epoch 142/500\n",
      "144/144 [==============================] - 0s 160us/step - loss: 1.0546 - acc: 0.5139 - val_loss: 1.0632 - val_acc: 0.5833\n",
      "Epoch 143/500\n",
      "144/144 [==============================] - 0s 144us/step - loss: 1.0532 - acc: 0.5278 - val_loss: 1.0621 - val_acc: 0.5833\n",
      "Epoch 144/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 1.0518 - acc: 0.5417 - val_loss: 1.0610 - val_acc: 0.5833\n",
      "Epoch 145/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 1.0504 - acc: 0.5694 - val_loss: 1.0599 - val_acc: 0.5833\n",
      "Epoch 146/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 1.0490 - acc: 0.5764 - val_loss: 1.0588 - val_acc: 0.5833\n",
      "Epoch 147/500\n",
      "144/144 [==============================] - 0s 164us/step - loss: 1.0476 - acc: 0.5903 - val_loss: 1.0577 - val_acc: 0.5833\n",
      "Epoch 148/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 1.0463 - acc: 0.5903 - val_loss: 1.0567 - val_acc: 0.5938\n",
      "Epoch 149/500\n",
      "144/144 [==============================] - 0s 161us/step - loss: 1.0450 - acc: 0.5972 - val_loss: 1.0556 - val_acc: 0.5938\n",
      "Epoch 150/500\n",
      "144/144 [==============================] - 0s 177us/step - loss: 1.0437 - acc: 0.6042 - val_loss: 1.0545 - val_acc: 0.5938\n",
      "Epoch 151/500\n",
      "144/144 [==============================] - 0s 178us/step - loss: 1.0423 - acc: 0.6042 - val_loss: 1.0535 - val_acc: 0.5938\n",
      "Epoch 152/500\n",
      "144/144 [==============================] - 0s 185us/step - loss: 1.0410 - acc: 0.6042 - val_loss: 1.0524 - val_acc: 0.5938\n",
      "Epoch 153/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 1.0397 - acc: 0.6042 - val_loss: 1.0514 - val_acc: 0.5938\n",
      "Epoch 154/500\n",
      "144/144 [==============================] - 0s 158us/step - loss: 1.0384 - acc: 0.6042 - val_loss: 1.0503 - val_acc: 0.5938\n",
      "Epoch 155/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 1.0371 - acc: 0.6042 - val_loss: 1.0493 - val_acc: 0.5938\n",
      "Epoch 156/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 1.0358 - acc: 0.6111 - val_loss: 1.0482 - val_acc: 0.5938\n",
      "Epoch 157/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 1.0345 - acc: 0.6111 - val_loss: 1.0471 - val_acc: 0.5938\n",
      "Epoch 158/500\n",
      "144/144 [==============================] - 0s 161us/step - loss: 1.0332 - acc: 0.6111 - val_loss: 1.0461 - val_acc: 0.5938\n",
      "Epoch 159/500\n",
      "144/144 [==============================] - 0s 154us/step - loss: 1.0319 - acc: 0.6111 - val_loss: 1.0451 - val_acc: 0.5938\n",
      "Epoch 160/500\n",
      "144/144 [==============================] - 0s 159us/step - loss: 1.0307 - acc: 0.6111 - val_loss: 1.0441 - val_acc: 0.5938\n",
      "Epoch 161/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 1.0294 - acc: 0.6111 - val_loss: 1.0431 - val_acc: 0.5938\n",
      "Epoch 162/500\n",
      "144/144 [==============================] - 0s 175us/step - loss: 1.0282 - acc: 0.6111 - val_loss: 1.0420 - val_acc: 0.5938\n",
      "Epoch 163/500\n",
      "144/144 [==============================] - 0s 185us/step - loss: 1.0269 - acc: 0.6111 - val_loss: 1.0410 - val_acc: 0.5938\n",
      "Epoch 164/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 1.0256 - acc: 0.6111 - val_loss: 1.0399 - val_acc: 0.5938\n",
      "Epoch 165/500\n",
      "144/144 [==============================] - 0s 148us/step - loss: 1.0243 - acc: 0.6111 - val_loss: 1.0389 - val_acc: 0.5938\n",
      "Epoch 166/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 1.0230 - acc: 0.6111 - val_loss: 1.0379 - val_acc: 0.5938\n",
      "Epoch 167/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 1.0218 - acc: 0.6181 - val_loss: 1.0368 - val_acc: 0.5938\n",
      "Epoch 168/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 1.0205 - acc: 0.6181 - val_loss: 1.0358 - val_acc: 0.5938\n",
      "Epoch 169/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 1.0193 - acc: 0.6181 - val_loss: 1.0348 - val_acc: 0.5938\n",
      "Epoch 170/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 1.0180 - acc: 0.6181 - val_loss: 1.0338 - val_acc: 0.5938\n",
      "Epoch 171/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 1.0168 - acc: 0.6181 - val_loss: 1.0329 - val_acc: 0.5938\n",
      "Epoch 172/500\n",
      "144/144 [==============================] - 0s 161us/step - loss: 1.0156 - acc: 0.6181 - val_loss: 1.0319 - val_acc: 0.5938\n",
      "Epoch 173/500\n",
      "144/144 [==============================] - 0s 161us/step - loss: 1.0144 - acc: 0.6181 - val_loss: 1.0308 - val_acc: 0.5938\n",
      "Epoch 174/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 1.0131 - acc: 0.6181 - val_loss: 1.0299 - val_acc: 0.5938\n",
      "Epoch 175/500\n",
      "144/144 [==============================] - 0s 177us/step - loss: 1.0119 - acc: 0.6181 - val_loss: 1.0289 - val_acc: 0.5938\n",
      "Epoch 176/500\n",
      "144/144 [==============================] - 0s 177us/step - loss: 1.0107 - acc: 0.6181 - val_loss: 1.0279 - val_acc: 0.5938\n",
      "Epoch 177/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 1.0095 - acc: 0.6181 - val_loss: 1.0269 - val_acc: 0.5938\n",
      "Epoch 178/500\n",
      "144/144 [==============================] - 0s 196us/step - loss: 1.0083 - acc: 0.6181 - val_loss: 1.0259 - val_acc: 0.5938\n",
      "Epoch 179/500\n",
      "144/144 [==============================] - 0s 190us/step - loss: 1.0071 - acc: 0.6181 - val_loss: 1.0250 - val_acc: 0.5938\n",
      "Epoch 180/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 1.0060 - acc: 0.6181 - val_loss: 1.0241 - val_acc: 0.5938\n",
      "Epoch 181/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 1.0049 - acc: 0.6181 - val_loss: 1.0232 - val_acc: 0.5938\n",
      "Epoch 182/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 1.0037 - acc: 0.6181 - val_loss: 1.0222 - val_acc: 0.5938\n",
      "Epoch 183/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 1.0026 - acc: 0.6181 - val_loss: 1.0213 - val_acc: 0.5938\n",
      "Epoch 184/500\n",
      "144/144 [==============================] - 0s 200us/step - loss: 1.0015 - acc: 0.6181 - val_loss: 1.0204 - val_acc: 0.5938\n",
      "Epoch 185/500\n",
      "144/144 [==============================] - 0s 158us/step - loss: 1.0003 - acc: 0.6181 - val_loss: 1.0195 - val_acc: 0.5938\n",
      "Epoch 186/500\n",
      "144/144 [==============================] - 0s 160us/step - loss: 0.9993 - acc: 0.6181 - val_loss: 1.0187 - val_acc: 0.6042\n",
      "Epoch 187/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 0.9982 - acc: 0.6181 - val_loss: 1.0178 - val_acc: 0.6042\n",
      "Epoch 188/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 0.9971 - acc: 0.6181 - val_loss: 1.0168 - val_acc: 0.6042\n",
      "Epoch 189/500\n",
      "144/144 [==============================] - 0s 178us/step - loss: 0.9959 - acc: 0.6181 - val_loss: 1.0159 - val_acc: 0.6042\n",
      "Epoch 190/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 0.9948 - acc: 0.6181 - val_loss: 1.0151 - val_acc: 0.6042\n",
      "Epoch 191/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.9937 - acc: 0.6181 - val_loss: 1.0142 - val_acc: 0.6042\n",
      "Epoch 192/500\n",
      "144/144 [==============================] - 0s 175us/step - loss: 0.9927 - acc: 0.6181 - val_loss: 1.0133 - val_acc: 0.6042\n",
      "Epoch 193/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.9916 - acc: 0.6181 - val_loss: 1.0124 - val_acc: 0.6146\n",
      "Epoch 194/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.9905 - acc: 0.6181 - val_loss: 1.0116 - val_acc: 0.6146\n",
      "Epoch 195/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.9894 - acc: 0.6181 - val_loss: 1.0107 - val_acc: 0.6146\n",
      "Epoch 196/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.9883 - acc: 0.6181 - val_loss: 1.0098 - val_acc: 0.6146\n",
      "Epoch 197/500\n",
      "144/144 [==============================] - 0s 177us/step - loss: 0.9873 - acc: 0.6181 - val_loss: 1.0089 - val_acc: 0.6146\n",
      "Epoch 198/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 0.9862 - acc: 0.6181 - val_loss: 1.0081 - val_acc: 0.6146\n",
      "Epoch 199/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.9852 - acc: 0.6181 - val_loss: 1.0072 - val_acc: 0.6146\n",
      "Epoch 200/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.9841 - acc: 0.6181 - val_loss: 1.0064 - val_acc: 0.6146\n",
      "Epoch 201/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.9831 - acc: 0.6181 - val_loss: 1.0055 - val_acc: 0.6146\n",
      "Epoch 202/500\n",
      "144/144 [==============================] - 0s 177us/step - loss: 0.9821 - acc: 0.6181 - val_loss: 1.0047 - val_acc: 0.6146\n",
      "Epoch 203/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.9810 - acc: 0.6181 - val_loss: 1.0038 - val_acc: 0.6146\n",
      "Epoch 204/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.9799 - acc: 0.6181 - val_loss: 1.0030 - val_acc: 0.6146\n",
      "Epoch 205/500\n",
      "144/144 [==============================] - 0s 164us/step - loss: 0.9789 - acc: 0.6250 - val_loss: 1.0021 - val_acc: 0.6146\n",
      "Epoch 206/500\n",
      "144/144 [==============================] - 0s 186us/step - loss: 0.9778 - acc: 0.6250 - val_loss: 1.0012 - val_acc: 0.6146\n",
      "Epoch 207/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 0.9768 - acc: 0.6250 - val_loss: 1.0004 - val_acc: 0.6146\n",
      "Epoch 208/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.9758 - acc: 0.6250 - val_loss: 0.9996 - val_acc: 0.6146\n",
      "Epoch 209/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.9747 - acc: 0.6250 - val_loss: 0.9987 - val_acc: 0.6146\n",
      "Epoch 210/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.9737 - acc: 0.6250 - val_loss: 0.9979 - val_acc: 0.6146\n",
      "Epoch 211/500\n",
      "144/144 [==============================] - 0s 186us/step - loss: 0.9727 - acc: 0.6250 - val_loss: 0.9971 - val_acc: 0.6146\n",
      "Epoch 212/500\n",
      "144/144 [==============================] - 0s 181us/step - loss: 0.9717 - acc: 0.6250 - val_loss: 0.9963 - val_acc: 0.6146\n",
      "Epoch 213/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 0.9707 - acc: 0.6250 - val_loss: 0.9954 - val_acc: 0.6146\n",
      "Epoch 214/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 0.9697 - acc: 0.6319 - val_loss: 0.9946 - val_acc: 0.6146\n",
      "Epoch 215/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 0.9687 - acc: 0.6319 - val_loss: 0.9938 - val_acc: 0.6146\n",
      "Epoch 216/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.9677 - acc: 0.6319 - val_loss: 0.9930 - val_acc: 0.6146\n",
      "Epoch 217/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.9667 - acc: 0.6389 - val_loss: 0.9922 - val_acc: 0.6146\n",
      "Epoch 218/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.9657 - acc: 0.6389 - val_loss: 0.9914 - val_acc: 0.6146\n",
      "Epoch 219/500\n",
      "144/144 [==============================] - 0s 161us/step - loss: 0.9648 - acc: 0.6389 - val_loss: 0.9906 - val_acc: 0.6146\n",
      "Epoch 220/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.9638 - acc: 0.6389 - val_loss: 0.9898 - val_acc: 0.6146\n",
      "Epoch 221/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 0.9628 - acc: 0.6389 - val_loss: 0.9890 - val_acc: 0.6146\n",
      "Epoch 222/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.9619 - acc: 0.6389 - val_loss: 0.9882 - val_acc: 0.6146\n",
      "Epoch 223/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 0.9609 - acc: 0.6389 - val_loss: 0.9874 - val_acc: 0.6146\n",
      "Epoch 224/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.9599 - acc: 0.6389 - val_loss: 0.9866 - val_acc: 0.6146\n",
      "Epoch 225/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.9590 - acc: 0.6389 - val_loss: 0.9858 - val_acc: 0.6146\n",
      "Epoch 226/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.9580 - acc: 0.6389 - val_loss: 0.9850 - val_acc: 0.6146\n",
      "Epoch 227/500\n",
      "144/144 [==============================] - 0s 160us/step - loss: 0.9571 - acc: 0.6389 - val_loss: 0.9842 - val_acc: 0.6146\n",
      "Epoch 228/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.9561 - acc: 0.6389 - val_loss: 0.9834 - val_acc: 0.6146\n",
      "Epoch 229/500\n",
      "144/144 [==============================] - 0s 154us/step - loss: 0.9551 - acc: 0.6389 - val_loss: 0.9826 - val_acc: 0.6146\n",
      "Epoch 230/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.9541 - acc: 0.6389 - val_loss: 0.9818 - val_acc: 0.6146\n",
      "Epoch 231/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.9532 - acc: 0.6389 - val_loss: 0.9811 - val_acc: 0.6146\n",
      "Epoch 232/500\n",
      "144/144 [==============================] - 0s 164us/step - loss: 0.9523 - acc: 0.6389 - val_loss: 0.9803 - val_acc: 0.6250\n",
      "Epoch 233/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.9514 - acc: 0.6389 - val_loss: 0.9795 - val_acc: 0.6250\n",
      "Epoch 234/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.9504 - acc: 0.6389 - val_loss: 0.9787 - val_acc: 0.6250\n",
      "Epoch 235/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 0.9495 - acc: 0.6389 - val_loss: 0.9780 - val_acc: 0.6250\n",
      "Epoch 236/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 0.9485 - acc: 0.6389 - val_loss: 0.9772 - val_acc: 0.6250\n",
      "Epoch 237/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 0.9476 - acc: 0.6389 - val_loss: 0.9764 - val_acc: 0.6250\n",
      "Epoch 238/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 0.9467 - acc: 0.6389 - val_loss: 0.9757 - val_acc: 0.6250\n",
      "Epoch 239/500\n",
      "144/144 [==============================] - 0s 178us/step - loss: 0.9458 - acc: 0.6389 - val_loss: 0.9749 - val_acc: 0.6250\n",
      "Epoch 240/500\n",
      "144/144 [==============================] - 0s 164us/step - loss: 0.9448 - acc: 0.6389 - val_loss: 0.9741 - val_acc: 0.6250\n",
      "Epoch 241/500\n",
      "144/144 [==============================] - 0s 178us/step - loss: 0.9439 - acc: 0.6389 - val_loss: 0.9734 - val_acc: 0.6250\n",
      "Epoch 242/500\n",
      "144/144 [==============================] - 0s 161us/step - loss: 0.9430 - acc: 0.6389 - val_loss: 0.9726 - val_acc: 0.6250\n",
      "Epoch 243/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 0.9421 - acc: 0.6389 - val_loss: 0.9718 - val_acc: 0.6250\n",
      "Epoch 244/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 0.9412 - acc: 0.6389 - val_loss: 0.9711 - val_acc: 0.6250\n",
      "Epoch 245/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.9402 - acc: 0.6389 - val_loss: 0.9703 - val_acc: 0.6250\n",
      "Epoch 246/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 0.9393 - acc: 0.6389 - val_loss: 0.9696 - val_acc: 0.6250\n",
      "Epoch 247/500\n",
      "144/144 [==============================] - 0s 160us/step - loss: 0.9384 - acc: 0.6389 - val_loss: 0.9688 - val_acc: 0.6250\n",
      "Epoch 248/500\n",
      "144/144 [==============================] - 0s 160us/step - loss: 0.9375 - acc: 0.6389 - val_loss: 0.9681 - val_acc: 0.6250\n",
      "Epoch 249/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 0.9367 - acc: 0.6389 - val_loss: 0.9674 - val_acc: 0.6250\n",
      "Epoch 250/500\n",
      "144/144 [==============================] - 0s 177us/step - loss: 0.9358 - acc: 0.6389 - val_loss: 0.9666 - val_acc: 0.6250\n",
      "Epoch 251/500\n",
      "144/144 [==============================] - 0s 179us/step - loss: 0.9348 - acc: 0.6389 - val_loss: 0.9658 - val_acc: 0.6250\n",
      "Epoch 252/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.9340 - acc: 0.6389 - val_loss: 0.9651 - val_acc: 0.6250\n",
      "Epoch 253/500\n",
      "144/144 [==============================] - 0s 161us/step - loss: 0.9331 - acc: 0.6389 - val_loss: 0.9644 - val_acc: 0.6250\n",
      "Epoch 254/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.9322 - acc: 0.6389 - val_loss: 0.9636 - val_acc: 0.6250\n",
      "Epoch 255/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.9313 - acc: 0.6389 - val_loss: 0.9629 - val_acc: 0.6250\n",
      "Epoch 256/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.9304 - acc: 0.6389 - val_loss: 0.9622 - val_acc: 0.6250\n",
      "Epoch 257/500\n",
      "144/144 [==============================] - 0s 161us/step - loss: 0.9295 - acc: 0.6389 - val_loss: 0.9614 - val_acc: 0.6250\n",
      "Epoch 258/500\n",
      "144/144 [==============================] - 0s 151us/step - loss: 0.9287 - acc: 0.6389 - val_loss: 0.9607 - val_acc: 0.6250\n",
      "Epoch 259/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.9278 - acc: 0.6389 - val_loss: 0.9600 - val_acc: 0.6250\n",
      "Epoch 260/500\n",
      "144/144 [==============================] - 0s 229us/step - loss: 0.9269 - acc: 0.6389 - val_loss: 0.9593 - val_acc: 0.6250\n",
      "Epoch 261/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.9261 - acc: 0.6389 - val_loss: 0.9586 - val_acc: 0.6250\n",
      "Epoch 262/500\n",
      "144/144 [==============================] - 0s 164us/step - loss: 0.9252 - acc: 0.6389 - val_loss: 0.9578 - val_acc: 0.6250\n",
      "Epoch 263/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.9244 - acc: 0.6389 - val_loss: 0.9571 - val_acc: 0.6250\n",
      "Epoch 264/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.9235 - acc: 0.6389 - val_loss: 0.9564 - val_acc: 0.6250\n",
      "Epoch 265/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 0.9227 - acc: 0.6389 - val_loss: 0.9557 - val_acc: 0.6250\n",
      "Epoch 266/500\n",
      "144/144 [==============================] - 0s 175us/step - loss: 0.9218 - acc: 0.6389 - val_loss: 0.9550 - val_acc: 0.6250\n",
      "Epoch 267/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.9210 - acc: 0.6389 - val_loss: 0.9543 - val_acc: 0.6250\n",
      "Epoch 268/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 0.9201 - acc: 0.6389 - val_loss: 0.9536 - val_acc: 0.6250\n",
      "Epoch 269/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.9193 - acc: 0.6389 - val_loss: 0.9529 - val_acc: 0.6250\n",
      "Epoch 270/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.9185 - acc: 0.6389 - val_loss: 0.9522 - val_acc: 0.6250\n",
      "Epoch 271/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 0.9177 - acc: 0.6389 - val_loss: 0.9515 - val_acc: 0.6250\n",
      "Epoch 272/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.9168 - acc: 0.6389 - val_loss: 0.9508 - val_acc: 0.6250\n",
      "Epoch 273/500\n",
      "144/144 [==============================] - 0s 175us/step - loss: 0.9160 - acc: 0.6389 - val_loss: 0.9501 - val_acc: 0.6250\n",
      "Epoch 274/500\n",
      "144/144 [==============================] - 0s 179us/step - loss: 0.9152 - acc: 0.6389 - val_loss: 0.9494 - val_acc: 0.6250\n",
      "Epoch 275/500\n",
      "144/144 [==============================] - 0s 178us/step - loss: 0.9144 - acc: 0.6389 - val_loss: 0.9487 - val_acc: 0.6250\n",
      "Epoch 276/500\n",
      "144/144 [==============================] - 0s 158us/step - loss: 0.9136 - acc: 0.6389 - val_loss: 0.9480 - val_acc: 0.6250\n",
      "Epoch 277/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.9127 - acc: 0.6389 - val_loss: 0.9473 - val_acc: 0.6250\n",
      "Epoch 278/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.9119 - acc: 0.6389 - val_loss: 0.9466 - val_acc: 0.6250\n",
      "Epoch 279/500\n",
      "144/144 [==============================] - 0s 175us/step - loss: 0.9110 - acc: 0.6389 - val_loss: 0.9459 - val_acc: 0.6250\n",
      "Epoch 280/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.9103 - acc: 0.6389 - val_loss: 0.9453 - val_acc: 0.6250\n",
      "Epoch 281/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.9094 - acc: 0.6389 - val_loss: 0.9446 - val_acc: 0.6250\n",
      "Epoch 282/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 0.9086 - acc: 0.6389 - val_loss: 0.9439 - val_acc: 0.6250\n",
      "Epoch 283/500\n",
      "144/144 [==============================] - 0s 179us/step - loss: 0.9078 - acc: 0.6389 - val_loss: 0.9432 - val_acc: 0.6250\n",
      "Epoch 284/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.9071 - acc: 0.6389 - val_loss: 0.9426 - val_acc: 0.6250\n",
      "Epoch 285/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.9063 - acc: 0.6389 - val_loss: 0.9419 - val_acc: 0.6250\n",
      "Epoch 286/500\n",
      "144/144 [==============================] - 0s 155us/step - loss: 0.9055 - acc: 0.6389 - val_loss: 0.9412 - val_acc: 0.6354\n",
      "Epoch 287/500\n",
      "144/144 [==============================] - 0s 159us/step - loss: 0.9047 - acc: 0.6389 - val_loss: 0.9406 - val_acc: 0.6354\n",
      "Epoch 288/500\n",
      "144/144 [==============================] - 0s 151us/step - loss: 0.9039 - acc: 0.6389 - val_loss: 0.9399 - val_acc: 0.6354\n",
      "Epoch 289/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 0.9032 - acc: 0.6389 - val_loss: 0.9392 - val_acc: 0.6354\n",
      "Epoch 290/500\n",
      "144/144 [==============================] - 0s 159us/step - loss: 0.9024 - acc: 0.6389 - val_loss: 0.9386 - val_acc: 0.6354\n",
      "Epoch 291/500\n",
      "144/144 [==============================] - 0s 177us/step - loss: 0.9016 - acc: 0.6389 - val_loss: 0.9379 - val_acc: 0.6354\n",
      "Epoch 292/500\n",
      "144/144 [==============================] - 0s 191us/step - loss: 0.9008 - acc: 0.6389 - val_loss: 0.9373 - val_acc: 0.6354\n",
      "Epoch 293/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.9001 - acc: 0.6389 - val_loss: 0.9366 - val_acc: 0.6354\n",
      "Epoch 294/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.8993 - acc: 0.6389 - val_loss: 0.9360 - val_acc: 0.6458\n",
      "Epoch 295/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8985 - acc: 0.6389 - val_loss: 0.9353 - val_acc: 0.6458\n",
      "Epoch 296/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.8977 - acc: 0.6389 - val_loss: 0.9347 - val_acc: 0.6458\n",
      "Epoch 297/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.8970 - acc: 0.6389 - val_loss: 0.9340 - val_acc: 0.6458\n",
      "Epoch 298/500\n",
      "144/144 [==============================] - 0s 164us/step - loss: 0.8962 - acc: 0.6389 - val_loss: 0.9334 - val_acc: 0.6458\n",
      "Epoch 299/500\n",
      "144/144 [==============================] - 0s 155us/step - loss: 0.8955 - acc: 0.6389 - val_loss: 0.9327 - val_acc: 0.6458\n",
      "Epoch 300/500\n",
      "144/144 [==============================] - 0s 175us/step - loss: 0.8947 - acc: 0.6458 - val_loss: 0.9321 - val_acc: 0.6458\n",
      "Epoch 301/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 0.8939 - acc: 0.6458 - val_loss: 0.9314 - val_acc: 0.6458\n",
      "Epoch 302/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8932 - acc: 0.6458 - val_loss: 0.9308 - val_acc: 0.6458\n",
      "Epoch 303/500\n",
      "144/144 [==============================] - 0s 184us/step - loss: 0.8924 - acc: 0.6458 - val_loss: 0.9301 - val_acc: 0.6458\n",
      "Epoch 304/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.8917 - acc: 0.6458 - val_loss: 0.9295 - val_acc: 0.6458\n",
      "Epoch 305/500\n",
      "144/144 [==============================] - 0s 157us/step - loss: 0.8909 - acc: 0.6458 - val_loss: 0.9289 - val_acc: 0.6458\n",
      "Epoch 306/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.8902 - acc: 0.6458 - val_loss: 0.9282 - val_acc: 0.6458\n",
      "Epoch 307/500\n",
      "144/144 [==============================] - 0s 159us/step - loss: 0.8895 - acc: 0.6458 - val_loss: 0.9276 - val_acc: 0.6458\n",
      "Epoch 308/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.8887 - acc: 0.6458 - val_loss: 0.9269 - val_acc: 0.6458\n",
      "Epoch 309/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.8880 - acc: 0.6458 - val_loss: 0.9263 - val_acc: 0.6458\n",
      "Epoch 310/500\n",
      "144/144 [==============================] - 0s 155us/step - loss: 0.8873 - acc: 0.6458 - val_loss: 0.9257 - val_acc: 0.6458\n",
      "Epoch 311/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 0.8865 - acc: 0.6389 - val_loss: 0.9251 - val_acc: 0.6458\n",
      "Epoch 312/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 0.8858 - acc: 0.6389 - val_loss: 0.9245 - val_acc: 0.6458\n",
      "Epoch 313/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.8851 - acc: 0.6389 - val_loss: 0.9238 - val_acc: 0.6458\n",
      "Epoch 314/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 0.8844 - acc: 0.6389 - val_loss: 0.9232 - val_acc: 0.6458\n",
      "Epoch 315/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 0.8836 - acc: 0.6389 - val_loss: 0.9226 - val_acc: 0.6458\n",
      "Epoch 316/500\n",
      "144/144 [==============================] - 0s 185us/step - loss: 0.8829 - acc: 0.6389 - val_loss: 0.9220 - val_acc: 0.6458\n",
      "Epoch 317/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8822 - acc: 0.6389 - val_loss: 0.9214 - val_acc: 0.6458\n",
      "Epoch 318/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8815 - acc: 0.6389 - val_loss: 0.9208 - val_acc: 0.6458\n",
      "Epoch 319/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.8808 - acc: 0.6389 - val_loss: 0.9201 - val_acc: 0.6458\n",
      "Epoch 320/500\n",
      "144/144 [==============================] - 0s 159us/step - loss: 0.8801 - acc: 0.6389 - val_loss: 0.9195 - val_acc: 0.6458\n",
      "Epoch 321/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8794 - acc: 0.6389 - val_loss: 0.9189 - val_acc: 0.6458\n",
      "Epoch 322/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 0.8786 - acc: 0.6389 - val_loss: 0.9183 - val_acc: 0.6458\n",
      "Epoch 323/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.8779 - acc: 0.6389 - val_loss: 0.9177 - val_acc: 0.6458\n",
      "Epoch 324/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.8772 - acc: 0.6389 - val_loss: 0.9171 - val_acc: 0.6458\n",
      "Epoch 325/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 0.8765 - acc: 0.6389 - val_loss: 0.9165 - val_acc: 0.6458\n",
      "Epoch 326/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.8758 - acc: 0.6389 - val_loss: 0.9159 - val_acc: 0.6458\n",
      "Epoch 327/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 0.8751 - acc: 0.6389 - val_loss: 0.9153 - val_acc: 0.6458\n",
      "Epoch 328/500\n",
      "144/144 [==============================] - 0s 202us/step - loss: 0.8744 - acc: 0.6389 - val_loss: 0.9147 - val_acc: 0.6458\n",
      "Epoch 329/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 0.8737 - acc: 0.6389 - val_loss: 0.9141 - val_acc: 0.6458\n",
      "Epoch 330/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 0.8730 - acc: 0.6389 - val_loss: 0.9135 - val_acc: 0.6458\n",
      "Epoch 331/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.8723 - acc: 0.6389 - val_loss: 0.9129 - val_acc: 0.6458\n",
      "Epoch 332/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.8716 - acc: 0.6389 - val_loss: 0.9123 - val_acc: 0.6458\n",
      "Epoch 333/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.8710 - acc: 0.6389 - val_loss: 0.9117 - val_acc: 0.6458\n",
      "Epoch 334/500\n",
      "144/144 [==============================] - 0s 161us/step - loss: 0.8703 - acc: 0.6389 - val_loss: 0.9111 - val_acc: 0.6458\n",
      "Epoch 335/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.8696 - acc: 0.6458 - val_loss: 0.9105 - val_acc: 0.6458\n",
      "Epoch 336/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 0.8689 - acc: 0.6458 - val_loss: 0.9099 - val_acc: 0.6458\n",
      "Epoch 337/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 0.8682 - acc: 0.6458 - val_loss: 0.9093 - val_acc: 0.6458\n",
      "Epoch 338/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.8676 - acc: 0.6458 - val_loss: 0.9087 - val_acc: 0.6458\n",
      "Epoch 339/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 0.8669 - acc: 0.6458 - val_loss: 0.9082 - val_acc: 0.6458\n",
      "Epoch 340/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 0.8662 - acc: 0.6458 - val_loss: 0.9076 - val_acc: 0.6458\n",
      "Epoch 341/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.8655 - acc: 0.6458 - val_loss: 0.9070 - val_acc: 0.6458\n",
      "Epoch 342/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 0.8648 - acc: 0.6458 - val_loss: 0.9064 - val_acc: 0.6458\n",
      "Epoch 343/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8642 - acc: 0.6458 - val_loss: 0.9059 - val_acc: 0.6458\n",
      "Epoch 344/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8635 - acc: 0.6458 - val_loss: 0.9053 - val_acc: 0.6458\n",
      "Epoch 345/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.8629 - acc: 0.6458 - val_loss: 0.9047 - val_acc: 0.6458\n",
      "Epoch 346/500\n",
      "144/144 [==============================] - 0s 157us/step - loss: 0.8622 - acc: 0.6458 - val_loss: 0.9041 - val_acc: 0.6458\n",
      "Epoch 347/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 0.8616 - acc: 0.6458 - val_loss: 0.9036 - val_acc: 0.6458\n",
      "Epoch 348/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 0.8609 - acc: 0.6458 - val_loss: 0.9030 - val_acc: 0.6458\n",
      "Epoch 349/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8602 - acc: 0.6458 - val_loss: 0.9024 - val_acc: 0.6458\n",
      "Epoch 350/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.8596 - acc: 0.6458 - val_loss: 0.9019 - val_acc: 0.6458\n",
      "Epoch 351/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8589 - acc: 0.6458 - val_loss: 0.9013 - val_acc: 0.6458\n",
      "Epoch 352/500\n",
      "144/144 [==============================] - 0s 159us/step - loss: 0.8583 - acc: 0.6458 - val_loss: 0.9007 - val_acc: 0.6458\n",
      "Epoch 353/500\n",
      "144/144 [==============================] - 0s 157us/step - loss: 0.8576 - acc: 0.6458 - val_loss: 0.9002 - val_acc: 0.6458\n",
      "Epoch 354/500\n",
      "144/144 [==============================] - 0s 177us/step - loss: 0.8570 - acc: 0.6458 - val_loss: 0.8996 - val_acc: 0.6458\n",
      "Epoch 355/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.8564 - acc: 0.6458 - val_loss: 0.8991 - val_acc: 0.6458\n",
      "Epoch 356/500\n",
      "144/144 [==============================] - 0s 180us/step - loss: 0.8557 - acc: 0.6458 - val_loss: 0.8985 - val_acc: 0.6458\n",
      "Epoch 357/500\n",
      "144/144 [==============================] - 0s 207us/step - loss: 0.8551 - acc: 0.6458 - val_loss: 0.8980 - val_acc: 0.6458\n",
      "Epoch 358/500\n",
      "144/144 [==============================] - 0s 161us/step - loss: 0.8544 - acc: 0.6458 - val_loss: 0.8974 - val_acc: 0.6458\n",
      "Epoch 359/500\n",
      "144/144 [==============================] - 0s 153us/step - loss: 0.8538 - acc: 0.6458 - val_loss: 0.8968 - val_acc: 0.6458\n",
      "Epoch 360/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.8532 - acc: 0.6458 - val_loss: 0.8963 - val_acc: 0.6458\n",
      "Epoch 361/500\n",
      "144/144 [==============================] - 0s 187us/step - loss: 0.8525 - acc: 0.6458 - val_loss: 0.8957 - val_acc: 0.6458\n",
      "Epoch 362/500\n",
      "144/144 [==============================] - 0s 186us/step - loss: 0.8519 - acc: 0.6458 - val_loss: 0.8952 - val_acc: 0.6458\n",
      "Epoch 363/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 0.8513 - acc: 0.6458 - val_loss: 0.8946 - val_acc: 0.6458\n",
      "Epoch 364/500\n",
      "144/144 [==============================] - 0s 177us/step - loss: 0.8507 - acc: 0.6458 - val_loss: 0.8941 - val_acc: 0.6458\n",
      "Epoch 365/500\n",
      "144/144 [==============================] - 0s 175us/step - loss: 0.8500 - acc: 0.6458 - val_loss: 0.8936 - val_acc: 0.6458\n",
      "Epoch 366/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 0.8494 - acc: 0.6458 - val_loss: 0.8930 - val_acc: 0.6458\n",
      "Epoch 367/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.8488 - acc: 0.6458 - val_loss: 0.8925 - val_acc: 0.6458\n",
      "Epoch 368/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 0.8482 - acc: 0.6458 - val_loss: 0.8919 - val_acc: 0.6458\n",
      "Epoch 369/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 0.8475 - acc: 0.6458 - val_loss: 0.8914 - val_acc: 0.6458\n",
      "Epoch 370/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.8469 - acc: 0.6458 - val_loss: 0.8908 - val_acc: 0.6458\n",
      "Epoch 371/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.8463 - acc: 0.6458 - val_loss: 0.8903 - val_acc: 0.6458\n",
      "Epoch 372/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.8457 - acc: 0.6458 - val_loss: 0.8898 - val_acc: 0.6458\n",
      "Epoch 373/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 0.8451 - acc: 0.6458 - val_loss: 0.8892 - val_acc: 0.6458\n",
      "Epoch 374/500\n",
      "144/144 [==============================] - 0s 180us/step - loss: 0.8444 - acc: 0.6458 - val_loss: 0.8887 - val_acc: 0.6458\n",
      "Epoch 375/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.8438 - acc: 0.6458 - val_loss: 0.8882 - val_acc: 0.6458\n",
      "Epoch 376/500\n",
      "144/144 [==============================] - 0s 175us/step - loss: 0.8432 - acc: 0.6458 - val_loss: 0.8876 - val_acc: 0.6458\n",
      "Epoch 377/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 0.8426 - acc: 0.6458 - val_loss: 0.8871 - val_acc: 0.6458\n",
      "Epoch 378/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 0.8420 - acc: 0.6458 - val_loss: 0.8866 - val_acc: 0.6458\n",
      "Epoch 379/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 0.8414 - acc: 0.6458 - val_loss: 0.8860 - val_acc: 0.6458\n",
      "Epoch 380/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.8408 - acc: 0.6458 - val_loss: 0.8855 - val_acc: 0.6458\n",
      "Epoch 381/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.8402 - acc: 0.6458 - val_loss: 0.8850 - val_acc: 0.6458\n",
      "Epoch 382/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.8396 - acc: 0.6458 - val_loss: 0.8845 - val_acc: 0.6458\n",
      "Epoch 383/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 0.8390 - acc: 0.6458 - val_loss: 0.8839 - val_acc: 0.6458\n",
      "Epoch 384/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.8385 - acc: 0.6458 - val_loss: 0.8834 - val_acc: 0.6458\n",
      "Epoch 385/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.8378 - acc: 0.6458 - val_loss: 0.8829 - val_acc: 0.6458\n",
      "Epoch 386/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.8372 - acc: 0.6458 - val_loss: 0.8824 - val_acc: 0.6458\n",
      "Epoch 387/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.8366 - acc: 0.6458 - val_loss: 0.8818 - val_acc: 0.6458\n",
      "Epoch 388/500\n",
      "144/144 [==============================] - 0s 164us/step - loss: 0.8361 - acc: 0.6458 - val_loss: 0.8813 - val_acc: 0.6458\n",
      "Epoch 389/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8355 - acc: 0.6458 - val_loss: 0.8808 - val_acc: 0.6458\n",
      "Epoch 390/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.8349 - acc: 0.6458 - val_loss: 0.8803 - val_acc: 0.6458\n",
      "Epoch 391/500\n",
      "144/144 [==============================] - 0s 155us/step - loss: 0.8343 - acc: 0.6458 - val_loss: 0.8798 - val_acc: 0.6458\n",
      "Epoch 392/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 0.8337 - acc: 0.6458 - val_loss: 0.8793 - val_acc: 0.6458\n",
      "Epoch 393/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8331 - acc: 0.6458 - val_loss: 0.8788 - val_acc: 0.6458\n",
      "Epoch 394/500\n",
      "144/144 [==============================] - 0s 190us/step - loss: 0.8325 - acc: 0.6458 - val_loss: 0.8783 - val_acc: 0.6458\n",
      "Epoch 395/500\n",
      "144/144 [==============================] - 0s 158us/step - loss: 0.8319 - acc: 0.6458 - val_loss: 0.8778 - val_acc: 0.6458\n",
      "Epoch 396/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 0.8314 - acc: 0.6458 - val_loss: 0.8773 - val_acc: 0.6458\n",
      "Epoch 397/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.8308 - acc: 0.6458 - val_loss: 0.8768 - val_acc: 0.6458\n",
      "Epoch 398/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.8302 - acc: 0.6458 - val_loss: 0.8763 - val_acc: 0.6458\n",
      "Epoch 399/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8297 - acc: 0.6458 - val_loss: 0.8757 - val_acc: 0.6458\n",
      "Epoch 400/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8291 - acc: 0.6458 - val_loss: 0.8752 - val_acc: 0.6458\n",
      "Epoch 401/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 0.8285 - acc: 0.6458 - val_loss: 0.8747 - val_acc: 0.6458\n",
      "Epoch 402/500\n",
      "144/144 [==============================] - 0s 164us/step - loss: 0.8280 - acc: 0.6458 - val_loss: 0.8742 - val_acc: 0.6458\n",
      "Epoch 403/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.8274 - acc: 0.6458 - val_loss: 0.8738 - val_acc: 0.6458\n",
      "Epoch 404/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 0.8268 - acc: 0.6458 - val_loss: 0.8732 - val_acc: 0.6458\n",
      "Epoch 405/500\n",
      "144/144 [==============================] - 0s 161us/step - loss: 0.8263 - acc: 0.6458 - val_loss: 0.8728 - val_acc: 0.6458\n",
      "Epoch 406/500\n",
      "144/144 [==============================] - 0s 161us/step - loss: 0.8257 - acc: 0.6458 - val_loss: 0.8723 - val_acc: 0.6458\n",
      "Epoch 407/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.8252 - acc: 0.6458 - val_loss: 0.8718 - val_acc: 0.6458\n",
      "Epoch 408/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.8246 - acc: 0.6458 - val_loss: 0.8713 - val_acc: 0.6458\n",
      "Epoch 409/500\n",
      "144/144 [==============================] - 0s 155us/step - loss: 0.8240 - acc: 0.6458 - val_loss: 0.8708 - val_acc: 0.6458\n",
      "Epoch 410/500\n",
      "144/144 [==============================] - 0s 150us/step - loss: 0.8235 - acc: 0.6458 - val_loss: 0.8703 - val_acc: 0.6458\n",
      "Epoch 411/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.8229 - acc: 0.6458 - val_loss: 0.8698 - val_acc: 0.6458\n",
      "Epoch 412/500\n",
      "144/144 [==============================] - 0s 150us/step - loss: 0.8224 - acc: 0.6458 - val_loss: 0.8693 - val_acc: 0.6458\n",
      "Epoch 413/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.8218 - acc: 0.6458 - val_loss: 0.8688 - val_acc: 0.6458\n",
      "Epoch 414/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.8213 - acc: 0.6458 - val_loss: 0.8684 - val_acc: 0.6458\n",
      "Epoch 415/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.8207 - acc: 0.6458 - val_loss: 0.8679 - val_acc: 0.6458\n",
      "Epoch 416/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 0.8202 - acc: 0.6458 - val_loss: 0.8674 - val_acc: 0.6458\n",
      "Epoch 417/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 0.8196 - acc: 0.6458 - val_loss: 0.8669 - val_acc: 0.6458\n",
      "Epoch 418/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.8191 - acc: 0.6458 - val_loss: 0.8664 - val_acc: 0.6458\n",
      "Epoch 419/500\n",
      "144/144 [==============================] - 0s 158us/step - loss: 0.8186 - acc: 0.6458 - val_loss: 0.8660 - val_acc: 0.6458\n",
      "Epoch 420/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 0.8180 - acc: 0.6458 - val_loss: 0.8655 - val_acc: 0.6458\n",
      "Epoch 421/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.8175 - acc: 0.6458 - val_loss: 0.8650 - val_acc: 0.6458\n",
      "Epoch 422/500\n",
      "144/144 [==============================] - 0s 179us/step - loss: 0.8169 - acc: 0.6458 - val_loss: 0.8645 - val_acc: 0.6458\n",
      "Epoch 423/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.8164 - acc: 0.6458 - val_loss: 0.8640 - val_acc: 0.6458\n",
      "Epoch 424/500\n",
      "144/144 [==============================] - 0s 158us/step - loss: 0.8159 - acc: 0.6458 - val_loss: 0.8636 - val_acc: 0.6458\n",
      "Epoch 425/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8153 - acc: 0.6458 - val_loss: 0.8631 - val_acc: 0.6458\n",
      "Epoch 426/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 0.8148 - acc: 0.6458 - val_loss: 0.8626 - val_acc: 0.6458\n",
      "Epoch 427/500\n",
      "144/144 [==============================] - 0s 181us/step - loss: 0.8143 - acc: 0.6458 - val_loss: 0.8622 - val_acc: 0.6458\n",
      "Epoch 428/500\n",
      "144/144 [==============================] - 0s 191us/step - loss: 0.8138 - acc: 0.6458 - val_loss: 0.8617 - val_acc: 0.6458\n",
      "Epoch 429/500\n",
      "144/144 [==============================] - 0s 203us/step - loss: 0.8132 - acc: 0.6458 - val_loss: 0.8612 - val_acc: 0.6458\n",
      "Epoch 430/500\n",
      "144/144 [==============================] - 0s 190us/step - loss: 0.8127 - acc: 0.6458 - val_loss: 0.8607 - val_acc: 0.6458\n",
      "Epoch 431/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 0.8122 - acc: 0.6458 - val_loss: 0.8603 - val_acc: 0.6458\n",
      "Epoch 432/500\n",
      "144/144 [==============================] - 0s 160us/step - loss: 0.8117 - acc: 0.6458 - val_loss: 0.8598 - val_acc: 0.6458\n",
      "Epoch 433/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 0.8111 - acc: 0.6458 - val_loss: 0.8594 - val_acc: 0.6458\n",
      "Epoch 434/500\n",
      "144/144 [==============================] - 0s 175us/step - loss: 0.8106 - acc: 0.6458 - val_loss: 0.8589 - val_acc: 0.6458\n",
      "Epoch 435/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 0.8101 - acc: 0.6458 - val_loss: 0.8584 - val_acc: 0.6458\n",
      "Epoch 436/500\n",
      "144/144 [==============================] - 0s 189us/step - loss: 0.8096 - acc: 0.6458 - val_loss: 0.8580 - val_acc: 0.6458\n",
      "Epoch 437/500\n",
      "144/144 [==============================] - 0s 198us/step - loss: 0.8091 - acc: 0.6458 - val_loss: 0.8575 - val_acc: 0.6458\n",
      "Epoch 438/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 0.8086 - acc: 0.6458 - val_loss: 0.8571 - val_acc: 0.6458\n",
      "Epoch 439/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 0.8081 - acc: 0.6458 - val_loss: 0.8566 - val_acc: 0.6458\n",
      "Epoch 440/500\n",
      "144/144 [==============================] - 0s 164us/step - loss: 0.8076 - acc: 0.6458 - val_loss: 0.8562 - val_acc: 0.6458\n",
      "Epoch 441/500\n",
      "144/144 [==============================] - 0s 161us/step - loss: 0.8071 - acc: 0.6458 - val_loss: 0.8557 - val_acc: 0.6458\n",
      "Epoch 442/500\n",
      "144/144 [==============================] - 0s 160us/step - loss: 0.8066 - acc: 0.6458 - val_loss: 0.8553 - val_acc: 0.6458\n",
      "Epoch 443/500\n",
      "144/144 [==============================] - 0s 179us/step - loss: 0.8060 - acc: 0.6458 - val_loss: 0.8548 - val_acc: 0.6458\n",
      "Epoch 444/500\n",
      "144/144 [==============================] - 0s 180us/step - loss: 0.8056 - acc: 0.6458 - val_loss: 0.8544 - val_acc: 0.6458\n",
      "Epoch 445/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8051 - acc: 0.6458 - val_loss: 0.8539 - val_acc: 0.6458\n",
      "Epoch 446/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 0.8046 - acc: 0.6458 - val_loss: 0.8535 - val_acc: 0.6458\n",
      "Epoch 447/500\n",
      "144/144 [==============================] - 0s 155us/step - loss: 0.8041 - acc: 0.6458 - val_loss: 0.8530 - val_acc: 0.6458\n",
      "Epoch 448/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.8036 - acc: 0.6458 - val_loss: 0.8526 - val_acc: 0.6458\n",
      "Epoch 449/500\n",
      "144/144 [==============================] - 0s 160us/step - loss: 0.8031 - acc: 0.6458 - val_loss: 0.8521 - val_acc: 0.6458\n",
      "Epoch 450/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 0.8026 - acc: 0.6458 - val_loss: 0.8517 - val_acc: 0.6458\n",
      "Epoch 451/500\n",
      "144/144 [==============================] - 0s 182us/step - loss: 0.8021 - acc: 0.6458 - val_loss: 0.8512 - val_acc: 0.6562\n",
      "Epoch 452/500\n",
      "144/144 [==============================] - 0s 162us/step - loss: 0.8016 - acc: 0.6458 - val_loss: 0.8508 - val_acc: 0.6562\n",
      "Epoch 453/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.8011 - acc: 0.6458 - val_loss: 0.8503 - val_acc: 0.6562\n",
      "Epoch 454/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 0.8006 - acc: 0.6458 - val_loss: 0.8499 - val_acc: 0.6562\n",
      "Epoch 455/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.8001 - acc: 0.6458 - val_loss: 0.8494 - val_acc: 0.6562\n",
      "Epoch 456/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.7996 - acc: 0.6458 - val_loss: 0.8490 - val_acc: 0.6562\n",
      "Epoch 457/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.7991 - acc: 0.6458 - val_loss: 0.8486 - val_acc: 0.6562\n",
      "Epoch 458/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.7986 - acc: 0.6458 - val_loss: 0.8481 - val_acc: 0.6562\n",
      "Epoch 459/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.7981 - acc: 0.6458 - val_loss: 0.8477 - val_acc: 0.6562\n",
      "Epoch 460/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.7976 - acc: 0.6458 - val_loss: 0.8472 - val_acc: 0.6562\n",
      "Epoch 461/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 0.7971 - acc: 0.6458 - val_loss: 0.8468 - val_acc: 0.6562\n",
      "Epoch 462/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 0.7967 - acc: 0.6458 - val_loss: 0.8464 - val_acc: 0.6562\n",
      "Epoch 463/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.7962 - acc: 0.6458 - val_loss: 0.8459 - val_acc: 0.6562\n",
      "Epoch 464/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 0.7957 - acc: 0.6458 - val_loss: 0.8455 - val_acc: 0.6562\n",
      "Epoch 465/500\n",
      "144/144 [==============================] - 0s 198us/step - loss: 0.7952 - acc: 0.6458 - val_loss: 0.8451 - val_acc: 0.6562\n",
      "Epoch 466/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 0.7947 - acc: 0.6458 - val_loss: 0.8447 - val_acc: 0.6562\n",
      "Epoch 467/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 0.7943 - acc: 0.6458 - val_loss: 0.8442 - val_acc: 0.6562\n",
      "Epoch 468/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.7938 - acc: 0.6458 - val_loss: 0.8438 - val_acc: 0.6562\n",
      "Epoch 469/500\n",
      "144/144 [==============================] - 0s 179us/step - loss: 0.7933 - acc: 0.6458 - val_loss: 0.8434 - val_acc: 0.6562\n",
      "Epoch 470/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.7929 - acc: 0.6458 - val_loss: 0.8429 - val_acc: 0.6562\n",
      "Epoch 471/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.7924 - acc: 0.6458 - val_loss: 0.8425 - val_acc: 0.6562\n",
      "Epoch 472/500\n",
      "144/144 [==============================] - 0s 174us/step - loss: 0.7919 - acc: 0.6458 - val_loss: 0.8421 - val_acc: 0.6562\n",
      "Epoch 473/500\n",
      "144/144 [==============================] - 0s 164us/step - loss: 0.7914 - acc: 0.6458 - val_loss: 0.8417 - val_acc: 0.6562\n",
      "Epoch 474/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 0.7910 - acc: 0.6458 - val_loss: 0.8412 - val_acc: 0.6562\n",
      "Epoch 475/500\n",
      "144/144 [==============================] - 0s 177us/step - loss: 0.7905 - acc: 0.6458 - val_loss: 0.8408 - val_acc: 0.6562\n",
      "Epoch 476/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.7901 - acc: 0.6458 - val_loss: 0.8404 - val_acc: 0.6562\n",
      "Epoch 477/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 0.7896 - acc: 0.6458 - val_loss: 0.8400 - val_acc: 0.6562\n",
      "Epoch 478/500\n",
      "144/144 [==============================] - 0s 154us/step - loss: 0.7891 - acc: 0.6458 - val_loss: 0.8396 - val_acc: 0.6562\n",
      "Epoch 479/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.7887 - acc: 0.6458 - val_loss: 0.8392 - val_acc: 0.6562\n",
      "Epoch 480/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 0.7882 - acc: 0.6458 - val_loss: 0.8387 - val_acc: 0.6562\n",
      "Epoch 481/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.7878 - acc: 0.6458 - val_loss: 0.8383 - val_acc: 0.6562\n",
      "Epoch 482/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.7873 - acc: 0.6458 - val_loss: 0.8379 - val_acc: 0.6562\n",
      "Epoch 483/500\n",
      "144/144 [==============================] - 0s 163us/step - loss: 0.7869 - acc: 0.6458 - val_loss: 0.8375 - val_acc: 0.6562\n",
      "Epoch 484/500\n",
      "144/144 [==============================] - 0s 168us/step - loss: 0.7864 - acc: 0.6458 - val_loss: 0.8371 - val_acc: 0.6562\n",
      "Epoch 485/500\n",
      "144/144 [==============================] - 0s 166us/step - loss: 0.7859 - acc: 0.6458 - val_loss: 0.8367 - val_acc: 0.6562\n",
      "Epoch 486/500\n",
      "144/144 [==============================] - 0s 172us/step - loss: 0.7855 - acc: 0.6458 - val_loss: 0.8363 - val_acc: 0.6562\n",
      "Epoch 487/500\n",
      "144/144 [==============================] - 0s 170us/step - loss: 0.7850 - acc: 0.6458 - val_loss: 0.8359 - val_acc: 0.6562\n",
      "Epoch 488/500\n",
      "144/144 [==============================] - 0s 165us/step - loss: 0.7846 - acc: 0.6458 - val_loss: 0.8354 - val_acc: 0.6562\n",
      "Epoch 489/500\n",
      "144/144 [==============================] - 0s 150us/step - loss: 0.7841 - acc: 0.6458 - val_loss: 0.8350 - val_acc: 0.6562\n",
      "Epoch 490/500\n",
      "144/144 [==============================] - 0s 178us/step - loss: 0.7837 - acc: 0.6458 - val_loss: 0.8346 - val_acc: 0.6562\n",
      "Epoch 491/500\n",
      "144/144 [==============================] - 0s 197us/step - loss: 0.7832 - acc: 0.6458 - val_loss: 0.8342 - val_acc: 0.6562\n",
      "Epoch 492/500\n",
      "144/144 [==============================] - 0s 169us/step - loss: 0.7828 - acc: 0.6458 - val_loss: 0.8338 - val_acc: 0.6562\n",
      "Epoch 493/500\n",
      "144/144 [==============================] - 0s 173us/step - loss: 0.7823 - acc: 0.6458 - val_loss: 0.8334 - val_acc: 0.6562\n",
      "Epoch 494/500\n",
      "144/144 [==============================] - 0s 175us/step - loss: 0.7819 - acc: 0.6458 - val_loss: 0.8330 - val_acc: 0.6562\n",
      "Epoch 495/500\n",
      "144/144 [==============================] - 0s 156us/step - loss: 0.7814 - acc: 0.6458 - val_loss: 0.8326 - val_acc: 0.6562\n",
      "Epoch 496/500\n",
      "144/144 [==============================] - 0s 180us/step - loss: 0.7810 - acc: 0.6458 - val_loss: 0.8322 - val_acc: 0.6562\n",
      "Epoch 497/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 0.7806 - acc: 0.6458 - val_loss: 0.8318 - val_acc: 0.6562\n",
      "Epoch 498/500\n",
      "144/144 [==============================] - 0s 176us/step - loss: 0.7801 - acc: 0.6458 - val_loss: 0.8314 - val_acc: 0.6562\n",
      "Epoch 499/500\n",
      "144/144 [==============================] - 0s 167us/step - loss: 0.7797 - acc: 0.6458 - val_loss: 0.8310 - val_acc: 0.6562\n",
      "Epoch 500/500\n",
      "144/144 [==============================] - 0s 171us/step - loss: 0.7793 - acc: 0.6458 - val_loss: 0.8306 - val_acc: 0.6562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[22,  0,  2],\n",
       "       [ 0,  0, 19],\n",
       "       [ 0,  0, 17]])"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nklRBhrskVZDhLkkVZLhLUgUZ7pJUQf8fHjFJ68doo2AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, mean_train, stddev_train,x_test,y_test,yn_test = trainModel(100, 42, 500)\n",
    "accuracy, CM = testModel(model, 10, mean_train, stddev_train,x_test,y_test)\n",
    "#testModel(model, Ntest, mean_train, stddev_train,x_test,y_test)\n",
    "CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZMxUn09uRCrc"
   },
   "outputs": [],
   "source": [
    "plotModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "dkj4meV_tUL9",
    "nbgrader": {
     "checksum": "1c367e3d2d36a80eb1fffbb27eb32da8",
     "grade": false,
     "grade_id": "cell-02b3b15dc3f435fe",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def testModel(model, Ntest, mean_train, stddev_train,x_test,y_test):\n",
    "    '''\n",
    "    generateXY for test, normalize, onehot, evaluate the model\n",
    "    Inputs:\n",
    "        model: trained Keras NN model\n",
    "        Ntest: int; number of test samples per class\n",
    "    Output:\n",
    "        accuracy: float; accuracy on the test data\n",
    "        CM: confusion matrix on the test data\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    loss, accuracy = model.evaluate(x_test,y_test,verbose=0)\n",
    "    pred_labels = model.predict(x_test)\n",
    "    \n",
    "    CM = confusion_matrix(yn_test, pred_labels.argmax(axis=1))\n",
    "\n",
    "    return accuracy, CM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21kDB-3vN8Go"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1WdkjiA2AZYj"
   },
   "source": [
    "# ADVANCED QUESTIONS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFBjZFLnA100"
   },
   "source": [
    "### Effect of changing Nh\n",
    "### Effect of changing Nepochs\n",
    "### Effect of changing N, no. of training samples\n",
    "\n",
    "Can you observe overfitting? \n",
    "\n",
    "Can you do hyperparameter tuning here? \n",
    "\n",
    "To normalize test data, why do we use the mean and stddev of training data?\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Abhay Katheria - 190608_kerasbasics_gaussian-assignment_f.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
